import numpy as np
import cPickle
def unPickle(file):
    fo = open(file,"rb")
    dict = cPickle.load(fo)
    fo.close()
    return dict

def dataSetNTestSet(addressData, addressTest, dictOfXteNYte=None):
    i = 1
    while(i<=5):
        addressNew = addressData + str(i)
        dictOfXtrNYtrs = unPickle(addressNew)
        XtrTem = np.array(dictOfXtrNYtrs['data'])
        YtrTem = np.array(dictOfXtrNYtrs['labels'])
        if (i == 1):
            Xtr = XtrTem
            Ytr = YtrTem
        else:
            Xtr = np.vstack((Xtr,XtrTem))
            Ytr = np.append(Ytr,YtrTem)
        i += 1

    #until here we build our data_set array
    #########
    #test set array

    dictOfXteNYte = unPickle(addressTest)
    Xte = np.array(dictOfXteNYte['data'])
    Yte = np.array(dictOfXteNYte['labels'])

    return Xtr, Ytr, Xte, Yte

def L_i (x,y,W):
    """
        - x is a column consist of photo pixels in a row
        - y is an integer [index] of the poto class and
        - W is the weight matrix

    """
    delta = 1.0
    scores = W.dot(x)
    correct_class_score = scores[y]
    D = W.shape[0]
    loss_i = 0.0
    for j in xrange(D):
        if j == y:
            continue
        loss_i += max(0, scores[j] - correct_class_score + delta)

    return loss_i/50000.

def L(Xtr, Ytr, W, loss=None):
    loss = L_i(Xtr[0],Ytr[0],W)
    for i in xrange(5000):
        loss = loss + L_i(Xtr[i],Ytr[i],W)
    return loss

def eval_numerical_gradient(f,x): ### fxh-fx/h
    # """" implementation of numerical gradient of f at x """
    #### f is a function that takes a single argument
    #### x is a point to evaluate the gradient at

    fx = f(x)
    grad = np.zeros(x.shape)
    h = 0.00001
    # iterate over all indexes in x
    it = np.nditer(x, flags = ['multi_index'], op_flags = ['readwrite'])

    while not it.finished:
        ix = it.multi_index
        old_value = x[ix]
        x[ix] = old_value + h
        fxh = f(x)
        x[ix] = old_value
        grad[ix] = (fxh - fx) / h
        it.iternext()

    return grad

def eval_numerical_gradient_2(f,x): ### fxh-fx/h
    # """" implementation of numerical gradient of f at x """
    #### f is a function that takes a single argument
    #### x is a point to evaluate the gradient at

    fx = f(x)
    grad = np.zeros(x.shape)
    h = 0.00001
    # iterate over all indexes in x
    it = np.nditer(x, flags = ['multi_index'], op_flags = ['readwrite'])

    while not it.finished:
        ix = it.multi_index
        old_value = x[ix]
        x[ix] = old_value - h
        fxh_N = f(x)
        x[ix] = old_value + 2*h
        fxh_P = f(x)
        x[ix] = old_value
        grad[ix] = (fxh_P - fxh_N) / (2*h)
        it.iternext()

    return grad

def CIFAR10_loss_fun (W):
    return L(Xtr, Ytr, W)





addressData = "data_batch_"
addressTest = "test_batch"
Xtr, Ytr, Xte, Yte = dataSetNTestSet( addressData, addressTest)

#### XtrN   is 50000,3073 that the new one is only filled with 1

XtrT = Xtr.T
i = np.ones((1,50000))
XtrT = np.vstack((XtrT,i))
XtrN = XtrT.T

print XtrN.shape
#strategy #1 using random numbers to find the best W

# bestloss = float ("inf")
# biggestInt = float ("inf")
# for num in xrange(1000):
#     W = np.random.randn(10,3073)/10000
#     loss = L(XtrN,Ytr,W) #### this is a loss function that calcs loss over all data
#     if loss < bestloss:
#         bestloss = loss
#         bestW = W
#
#     print 'in attempt %d the loss was %f, best %f' % (num, loss, bestloss)
# print bestW



#### that one sucks  :))

# strategy #2 kinda cool

# W = np.random.randn(10,3073)/100
# bestloss = float("inf")
# for i in xrange(1000):
#     stepSize = 0.0001
#     Wtry = W + np.random.randn(10,3073)/1000
#     loss = L(XtrN,Ytr,W)
#     if loss < bestloss:
#         W = Wtry
#         bestloss = loss
#     print 'iter %d loss is %f, and bestloss is %f' % (i, loss,bestloss)

############## edit your loss function add the regularization part and one over N ###############

# strategy #3

loss_original = CIFAR10_loss_fun(W)